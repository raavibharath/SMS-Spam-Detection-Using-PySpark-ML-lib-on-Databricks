{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b8178d3-1e23-47c3-a97a-2b171a151ba4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# DS-610 Week 6 Homework: Machine Learning on Apache Spark\n",
    " we will build a spam classifier on data stored on the Cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "704f0cdb-544b-48d7-be4b-c63f77d1cbbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from platform import python_version\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5067bf80-7e9c-4393-b657-741c41219903",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Loading Data\n",
    "Reminder: It is highly recommended that you try this homework on Saint Peters' Databricks. Depending on whether you are running on the cloud or locally, adjust the data source accordingly below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1136bc5c-495f-432e-95ee-3224611038a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+\n|label_string|                 sms|\n+------------+--------------------+\n|         ham|Go until jurong p...|\n|         ham|Ok lar... Joking ...|\n|        spam|Free entry in 2 a...|\n|         ham|U dun say so earl...|\n|         ham|Nah I don't think...|\n|        spam|FreeMsg Hey there...|\n|         ham|Even my brother i...|\n|         ham|As per your reque...|\n|        spam|WINNER!! As a val...|\n|        spam|Had your mobile 1...|\n+------------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "#Data source\n",
    "data_source = \"/FileStore/shared_uploads/dlee5@saintpeters.edu/ds610/SMSSpamCollection\"\n",
    "# Load data and rename column. DO NOT MODIFY\n",
    "df = spark.read.option(\"header\", \"false\") \\\n",
    "    .option(\"delimiter\", \"\\t\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(data_source) \\\n",
    "    .withColumnRenamed(\"_c0\", \"label_string\") \\\n",
    "    .withColumnRenamed(\"_c1\", \"sms\")\n",
    "df.limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e686b92-a40c-4eb0-825c-66e3bf3cd11f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- label_string: string (nullable = true)\n |-- sms: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35567ec4-f5c6-4d46-b9ef-9e07ae04d800",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 1: Pipelines\n",
    "First we will declare `pipeline_stages` which will hold the complete steps for getting our dataset into the format for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b87fc01-e1ae-4f3b-84c9-f396ad93dea5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "pipeline_stages = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d892cd9-632a-4d5e-aeac-da0d787eaf96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Part 1a\n",
    "We note that the `label_string` column consists of the label set `{ ham, spam }` (`ham` means not spam). Your task is now to create a pipeline stage which takes the input column `label_string` and outputs into a new column `label` which performs the following mapping: `{ham -> 0, spam -> 1}`. This is necessary since we would like to train a classifier on the training data.\n",
    "\n",
    "You may find the following useful:\n",
    "https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.StringIndexer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de3983b6-78e1-4ddd-bb26-489a6c593d81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert the \"label_string\" colun to 0/1 using StringIndexer.\n",
    "indexer = StringIndexer(inputCol=\"label_string\", outputCol=\"label\")\n",
    "# Add to the pipeline stage. DO NOT MODIFY.\n",
    "pipeline_stages.append(indexer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5951116-96aa-4f32-930d-ef23c608e974",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Part 1b\n",
    "Now we take a look at the `sms` column which represents the raw SMS message. We have to turn this into a more useful form. First we have to *tokenize* the message. For example:\n",
    "```\n",
    "The quick brown fox jumps over the lazy dog -> [ \"the\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\" ]\n",
    " You may find the following useful:\n",
    "https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.Tokenizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4e146a2-ae77-4c3b-92a7-bc76b5b88754",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Tokenize the \"sms\" column into the list of words under the column name \"words\" (inputCol=\"sms\" and outputCol=\"words\")\n",
    "# https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.Tokenizer.html\n",
    "tokenizer = Tokenizer(inputCol='sms',outputCol='words') \n",
    "\n",
    "# Add to the pipeline stage. DO NOT MODIFY.\n",
    "pipeline_stages.append(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05a841bc-1e66-41aa-bc54-d5f882d4a3d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Part 1c\n",
    "Finally, we create a feature vector called count vectorizer. For example:\n",
    "```\n",
    "The quick brown fox jumps over the lazy dog -> [ \"the\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\" ] -> (1000, [8, 10, 23, 34, 58, 100, 110, 112, 140], [2, 1, 1, 1, 1, 1, 1, 1])\n",
    "```\n",
    "where the first number denotes the total size of the vocabulary, the second list denotes the word index for each of the words in the original sentence, the third list denotes the corresponding count for each word in the original sentence.\n",
    "\n",
    "Your task is to create a count vector feature out of the `words` column under the column name `features`, using `(inputCol=\"words\", outputCol=\"features\")`.\n",
    "\n",
    "You may find this useful:\n",
    "https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.CountVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14eb2d35-d945-4d4c-91d9-ecf657c9cba3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Make a count vector feature out of the words column under the column name \"features\" (inputCol=\"words\", outputCol=\"features\")\n",
    "# https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.CountVectorizer.html\n",
    "cv = CountVectorizer(inputCol='words', outputCol='features',minDF=2.0)\n",
    "\n",
    "# Add to the pipeline stage. DO NOT MODIFY.\n",
    "pipeline_stages.append(cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cf0b930-d8bf-4974-a628-22a5fefc0e63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Interlude\n",
    "If your Part 1 is correctly implemented, then the following code should transform the data for model training. Note that for modeling training, only two of the columns in the transformed data is used, namely `label` and `features` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f75d5c2-de3b-455e-b9dd-179c4b49993d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY.\n",
    "pipeline = Pipeline(stages=pipeline_stages)\n",
    "data=pipeline.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f00607c-fd67-4456-b103-83c2c26c880c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+-----+--------------------+--------------------+\n|label_string|                 sms|label|               words|            features|\n+------------+--------------------+-----+--------------------+--------------------+\n|         ham|Go until jurong p...|  0.0|[go, until, juron...|(5461,[8,42,52,64...|\n|         ham|Ok lar... Joking ...|  0.0|[ok, lar..., joki...|(5461,[5,75,411,5...|\n|        spam|Free entry in 2 a...|  1.0|[free, entry, in,...|(5461,[0,3,8,20,5...|\n|         ham|U dun say so earl...|  0.0|[u, dun, say, so,...|(5461,[5,22,60,14...|\n|         ham|Nah I don't think...|  0.0|[nah, i, don't, t...|(5461,[0,1,66,87,...|\n|        spam|FreeMsg Hey there...|  1.0|[freemsg, hey, th...|(5461,[0,2,6,10,1...|\n|         ham|Even my brother i...|  0.0|[even, my, brothe...|(5461,[0,7,9,13,2...|\n|         ham|As per your reque...|  0.0|[as, per, your, r...|(5461,[0,10,11,44...|\n|        spam|WINNER!! As a val...|  1.0|[winner!!, as, a,...|(5461,[0,2,3,14,1...|\n|        spam|Had your mobile 1...|  1.0|[had, your, mobil...|(5461,[0,4,5,10,1...|\n|         ham|I'm gonna be home...|  0.0|[i'm, gonna, be, ...|(5461,[0,1,6,32,3...|\n|        spam|SIX chances to wi...|  1.0|[six, chances, to...|(5461,[0,6,40,46,...|\n|        spam|URGENT! You have ...|  1.0|[urgent!, you, ha...|(5461,[0,2,3,4,8,...|\n|         ham|I've been searchi...|  0.0|[i've, been, sear...|(5461,[0,1,2,3,4,...|\n|         ham|I HAVE A DATE ON ...|  0.0|[i, have, a, date...|(5461,[1,3,14,16,...|\n|        spam|XXXMobileMovieClu...|  1.0|[xxxmobilemoviecl...|(5461,[0,4,8,11,2...|\n|         ham|Oh k...i'm watchi...|  0.0|[oh, k...i'm, wat...|(5461,[158,314,46...|\n|         ham|Eh u remember how...|  0.0|[eh, u, remember,...|(5461,[1,5,20,47,...|\n|         ham|Fine if thats th...|  0.0|[fine, if, thats...|(5461,[4,5,29,59,...|\n|        spam|England v Macedon...|  1.0|[england, v, mace...|(5461,[0,4,28,82,...|\n+------------+--------------------+-----+--------------------+--------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea8e61eb-3e16-4884-bda7-764e5d39a21a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 2: Model Training\n",
    "Now we are ready to train out model. There are two parts to this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c96fee5f-5b20-4517-92a1-8f29f4988293",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Part 2a\n",
    "Let us first divide the `data` into `train` and `test` in the ratio of 0.8 to 0.2. You may find the following useful:\n",
    "https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.randomSplit.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9992f277-51c7-48c2-a1f4-9cdff3b34d43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Divide into train and test.\n",
    "train, test = data.randomSplit([0.8,0.2],seed=42)          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c21fb295-c577-4c0f-be50-909837ea4bc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Interlude\n",
    "The machine learning model we will be working with is called logistic regression. For your reference, the sample code for training is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3be82390-d154-49e3-b366-263235305e5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sample logistic regression code.\n",
    "lr = LogisticRegression()\n",
    "lrModel = lr.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acf29a46-2432-49d3-adf6-befa63d58807",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Part 2b\n",
    "In machine learning, we have to do what is a called hyperparameter tuning on a combination of parameters. For logistic regression, we can tune two hyperparameters, namely the $L_1$ regularizer and the $L_2$ regularizer in its elastic net formulation. For details on elastic net regularization, please see:\n",
    "https://en.wikipedia.org/wiki/Elastic_net_regularization\n",
    "\n",
    "The Wikipedia entry will just have a elastic net formulation of linear regression. For elastic net formulation of logistic regression, the cost function is a bit different and involves adding L1 and L2 regularization to cross entropy loss of the data. For more details, you may want to consult a machine learning textbook.\n",
    "\n",
    "Your task is here to finish the implementation that runs cross-validation on the set of elastic net regularization parameter below. Most of the skeleton code is provided for you, including setting up the parameter grid which can be plugged into the `CrossValidator`.\n",
    "\n",
    "For more details: https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.tuning.CrossValidator.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d707e6a-7fad-48cb-a087-8090b48af606",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Your code for Part 2b goes here.\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Create ParamGrid and Evaluator for Cross Validation\n",
    "paramGrid = ParamGridBuilder().baseOn({lr.maxIter: 100}).baseOn({lr.fitIntercept: False}).addGrid(lr.elasticNetParam, [0, 0.25, 0.5, 0.75, 1]).build()\n",
    "cvEvaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\")\n",
    "\n",
    "# Run Cross-validation\n",
    "cv = CrossValidator(estimator=lr, evaluator=cvEvaluator, estimatorParamMaps=paramGrid, numFolds=3)\n",
    "cvModel = cv.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20070a50-9f46-42ee-a7e3-d2b4a01ce4a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 3: Model Evaluation\n",
    "Evaluate the best model trained in Part 2 on the `test` set from Part 2a. You would need to have `cvModel` from the previous part perform `transform` the `test` set.\n",
    "\n",
    "For example:\n",
    "https://spark.apache.org/docs/latest/ml-classification-regression.html#binomial-logistic-regression\n",
    "\n",
    "Search for `# Make predictions` in this page for an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f14fb8cb-3e3a-4749-9989-94d0425e53f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(label_string='ham', sms=' &lt;DECIMAL&gt; m but its not a common car here so its better to buy from china or asia. Or if i find it less expensive. I.ll holla', label=0.0, words=['', '&lt;decimal&gt;', 'm', 'but', 'its', 'not', 'a', 'common', 'car', 'here', 'so', 'its', 'better', 'to', 'buy', 'from', 'china', 'or', 'asia.', 'or', 'if', 'i', 'find', 'it', 'less', 'expensive.', 'i.ll', 'holla'], features=SparseVector(5461, {0: 1.0, 1: 1.0, 3: 1.0, 19: 1.0, 21: 1.0, 22: 1.0, 23: 1.0, 24: 2.0, 25: 1.0, 29: 1.0, 40: 1.0, 59: 2.0, 145: 1.0, 161: 1.0, 199: 1.0, 308: 1.0, 330: 1.0, 396: 1.0, 467: 1.0, 1123: 1.0, 1143: 1.0, 1641: 1.0, 2805: 1.0, 3117: 1.0, 4443: 1.0}), rawPrediction=DenseVector([121.5116, -121.5116]), probability=DenseVector([1.0, 0.0]), prediction=0.0), Row(label_string='ham', sms=\" said kiss, kiss, i can't do the sound effects! He is a gorgeous man isn't he! Kind of person who needs a smile to brighten his day! \", label=0.0, words=['', 'said', 'kiss,', 'kiss,', 'i', \"can't\", 'do', 'the', 'sound', 'effects!', 'he', 'is', 'a', 'gorgeous', 'man', \"isn't\", 'he!', 'kind', 'of', 'person', 'who', 'needs', 'a', 'smile', 'to', 'brighten', 'his', 'day!'], features=SparseVector(5461, {0: 1.0, 1: 1.0, 3: 2.0, 4: 1.0, 7: 1.0, 12: 1.0, 21: 1.0, 36: 1.0, 66: 1.0, 109: 1.0, 171: 1.0, 195: 1.0, 207: 1.0, 315: 1.0, 340: 1.0, 343: 1.0, 654: 1.0, 700: 1.0, 970: 1.0, 1068: 1.0, 1466: 1.0, 4536: 1.0, 4839: 1.0}), rawPrediction=DenseVector([95.0803, -95.0803]), probability=DenseVector([1.0, 0.0]), prediction=0.0), Row(label_string='ham', sms=' what number do u live at? Is it 11?', label=0.0, words=['', 'what', 'number', 'do', 'u', 'live', 'at?', 'is', 'it', '11?'], features=SparseVector(5461, {5: 1.0, 7: 1.0, 19: 1.0, 21: 1.0, 36: 1.0, 48: 1.0, 166: 1.0, 281: 1.0, 3257: 1.0}), rawPrediction=DenseVector([67.7174, -67.7174]), probability=DenseVector([1.0, 0.0]), prediction=0.0), Row(label_string='ham', sms='\"Response\" is one of d powerful weapon 2 occupy a place in others \\'HEART\\'... So, always give response 2 who cares 4 U\"... Gud night..swt dreams..take care', label=0.0, words=['\"response\"', 'is', 'one', 'of', 'd', 'powerful', 'weapon', '2', 'occupy', 'a', 'place', 'in', 'others', \"'heart'...\", 'so,', 'always', 'give', 'response', '2', 'who', 'cares', '4', 'u\"...', 'gud', 'night..swt', 'dreams..take', 'care'], features=SparseVector(5461, {3: 1.0, 7: 1.0, 8: 1.0, 12: 1.0, 20: 2.0, 46: 1.0, 81: 1.0, 109: 1.0, 116: 1.0, 120: 1.0, 196: 1.0, 216: 1.0, 233: 1.0, 247: 1.0, 1192: 1.0, 1897: 1.0, 3324: 1.0, 3328: 1.0, 4334: 1.0}), rawPrediction=DenseVector([127.8608, -127.8608]), probability=DenseVector([1.0, 0.0]), prediction=0.0), Row(label_string='ham', sms='&lt;#&gt;  great loxahatchee xmas tree burning update: you can totally see stars here', label=0.0, words=['&lt;#&gt;', '', 'great', 'loxahatchee', 'xmas', 'tree', 'burning', 'update:', 'you', 'can', 'totally', 'see', 'stars', 'here'], features=SparseVector(5461, {2: 1.0, 21: 1.0, 27: 1.0, 41: 1.0, 84: 1.0, 132: 1.0, 145: 1.0, 472: 1.0, 1767: 1.0, 2083: 1.0, 2725: 1.0, 3755: 1.0, 4550: 1.0, 4855: 1.0}), rawPrediction=DenseVector([20.7268, -20.7268]), probability=DenseVector([1.0, 0.0]), prediction=0.0), Row(label_string='ham', sms=\"&lt;#&gt; , that's all? Guess that's easy enough\", label=0.0, words=['&lt;#&gt;', ',', \"that's\", 'all?', 'guess', \"that's\", 'easy', 'enough'], features=SparseVector(5461, {41: 1.0, 212: 2.0, 312: 1.0, 352: 1.0, 482: 1.0, 724: 1.0, 5442: 1.0}), rawPrediction=DenseVector([55.6778, -55.6778]), probability=DenseVector([1.0, 0.0]), prediction=0.0), Row(label_string='ham', sms=\"(No promises on when though, haven't even gotten dinner yet)\", label=0.0, words=['(no', 'promises', 'on', 'when', 'though,', \"haven't\", 'even', 'gotten', 'dinner', 'yet)'], features=SparseVector(5461, {16: 1.0, 39: 1.0, 182: 1.0, 442: 1.0, 489: 1.0, 2808: 1.0, 3046: 1.0, 5080: 1.0}), rawPrediction=DenseVector([-2.5896, 2.5896]), probability=DenseVector([0.0698, 0.9302]), prediction=1.0), Row(label_string='ham', sms=\"* Was a nice day and, impressively, i was sensible, went home early and now feel fine. Or am i just boring?! When's yours, i can't remember.\", label=0.0, words=['*', 'was', 'a', 'nice', 'day', 'and,', 'impressively,', 'i', 'was', 'sensible,', 'went', 'home', 'early', 'and', 'now', 'feel', 'fine.', 'or', 'am', 'i', 'just', 'boring?!', \"when's\", 'yours,', 'i', \"can't\", 'remember.'], features=SparseVector(5461, {1: 3.0, 3: 1.0, 6: 1.0, 24: 1.0, 35: 1.0, 49: 1.0, 54: 2.0, 58: 1.0, 105: 1.0, 108: 1.0, 188: 1.0, 200: 1.0, 207: 1.0, 255: 1.0, 290: 1.0, 438: 1.0, 697: 1.0, 2860: 1.0, 3533: 1.0, 5110: 1.0}), rawPrediction=DenseVector([112.1011, -112.1011]), probability=DenseVector([1.0, 0.0]), prediction=0.0), Row(label_string='ham', sms=', ow u dey.i paid 60,400thousad.i told  u would call . ', label=0.0, words=[',', 'ow', 'u', 'dey.i', 'paid', '60,400thousad.i', 'told', '', 'u', 'would', 'call', '.'], features=SparseVector(5461, {5: 2.0, 15: 1.0, 21: 1.0, 50: 1.0, 154: 1.0, 220: 1.0, 352: 1.0, 1807: 1.0}), rawPrediction=DenseVector([28.9117, -28.9117]), probability=DenseVector([1.0, 0.0]), prediction=0.0), Row(label_string='ham', sms='... Are you in the pub?', label=0.0, words=['...', 'are', 'you', 'in', 'the', 'pub?'], features=SparseVector(5461, {2: 1.0, 4: 1.0, 8: 1.0, 17: 1.0, 73: 1.0, 4534: 1.0}), rawPrediction=DenseVector([50.7977, -50.7977]), probability=DenseVector([1.0, 0.0]), prediction=0.0)]\nTest Area Under ROC:  0.9184446505875077\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on testData. cvModel uses the bestModel.\n",
    "cvPredictions = cvModel.transform(test)       \n",
    "print(cvPredictions.take(10))\n",
    "\n",
    "# Evaluate bestModel found from Cross Validation. DO NOT MODIFY.\n",
    "print (\"Test Area Under ROC: \", cvEvaluator.evaluate(cvPredictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fe19097-fe4c-48b6-98fe-9cf57fd50510",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Conclusion\n",
    "We are done! In order to see what parameter sets were explored during the grid search, we can run the command below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb44dc4c-8b2d-40e6-bc32-1e6f6acdc9d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{Param(parent='LogisticRegression_3c0c7995a540', name='aggregationDepth', doc='suggested depth for treeAggregate (>= 2).'): 2,\n",
       " Param(parent='LogisticRegression_3c0c7995a540', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.0,\n",
       " Param(parent='LogisticRegression_3c0c7995a540', name='family', doc='The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial'): 'auto',\n",
       " Param(parent='LogisticRegression_3c0c7995a540', name='featuresCol', doc='features column name.'): 'features',\n",
       " Param(parent='LogisticRegression_3c0c7995a540', name='fitIntercept', doc='whether to fit an intercept term.'): False,\n",
       " Param(parent='LogisticRegression_3c0c7995a540', name='labelCol', doc='label column name.'): 'label',\n",
       " Param(parent='LogisticRegression_3c0c7995a540', name='maxBlockSizeInMB', doc='maximum memory in MB for stacking input data into blocks. Data is stacked within partitions. If more than remaining data size in a partition then it is adjusted to the data size. Default 0.0 represents choosing optimal value, depends on specific algorithm. Must be >= 0.'): 0.0,\n",
       " Param(parent='LogisticRegression_3c0c7995a540', name='maxIter', doc='max number of iterations (>= 0).'): 100,\n",
       " Param(parent='LogisticRegression_3c0c7995a540', name='predictionCol', doc='prediction column name.'): 'prediction',\n",
       " Param(parent='LogisticRegression_3c0c7995a540', name='probabilityCol', doc='Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities.'): 'probability',\n",
       " Param(parent='LogisticRegression_3c0c7995a540', name='rawPredictionCol', doc='raw prediction (a.k.a. confidence) column name.'): 'rawPrediction',\n",
       " Param(parent='LogisticRegression_3c0c7995a540', name='regParam', doc='regularization parameter (>= 0).'): 0.0,\n",
       " Param(parent='LogisticRegression_3c0c7995a540', name='standardization', doc='whether to standardize the training features before fitting the model.'): True,\n",
       " Param(parent='LogisticRegression_3c0c7995a540', name='threshold', doc='Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p].'): 0.5,\n",
       " Param(parent='LogisticRegression_3c0c7995a540', name='tol', doc='the convergence tolerance for iterative algorithms (>= 0).'): 1e-06}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "cvModel.bestModel.extractParamMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a0c51b8-7ed3-4263-8885-78d46887955b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Spam_sms",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}